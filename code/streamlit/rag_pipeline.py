"""
MAPPO RAG ì¶”ë¡  íŒŒì´í”„ë¼ì¸
run_mappo.pyì˜ ì¶”ë¡  ê¸°ëŠ¥ì„ ìŠ¤íŠ¸ë¦¼ë¦¿ ì•±ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¶”ì¶œí•œ ëª¨ë“ˆ
ì‚¬ì „ í›ˆë ¨ëœ peter520416/llama1b-MMOA_RAG_Final_cp180 ëª¨ë¸ ì‚¬ìš©
"""

import os
import re
import json
import numpy as np
import torch
import torch.nn as nn
from pathlib import Path
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    logging
)
from sentence_transformers import SentenceTransformer, util
from rank_bm25 import BM25Okapi
from collections import defaultdict
from dotenv import load_dotenv
from huggingface_hub import login
from streamlit_config import Config  # ìŠ¤íŠ¸ë¦¼ë¦¿ ì „ìš© config ì‚¬ìš©

# ë¡œê¹… ì„¤ì •
logging.set_verbosity_error()

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ)
def find_project_root():
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    current = Path.cwd()
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ê°€ BOAZ_MP2ì¸ì§€ í™•ì¸
    if current.name == "BOAZ_MP2":
        return current
    
    # ìƒìœ„ ë””ë ‰í† ë¦¬ë“¤ì„ í™•ì¸
    for parent in current.parents:
        if parent.name == "BOAZ_MP2":
            return parent
    
    # ì°¾ì§€ ëª»í•œ ê²½ìš° í˜„ì¬ ë””ë ‰í† ë¦¬ ë°˜í™˜
    return current

project_root = find_project_root()
env_file = project_root / ".env"
if env_file.exists():
    load_dotenv(env_file)
else:
    load_dotenv()

HF_TOKEN = os.getenv("HF_TOKEN")
if HF_TOKEN:
    login(token=HF_TOKEN)

class RAGPipeline:
    """MAPPO RAG ì¶”ë¡  íŒŒì´í”„ë¼ì¸ - ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©"""
    
    def __init__(self):
        """ëª¨ë“  í•„ìš”í•œ ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.device = Config.DEVICE
        self._load_models()
        self._load_corpus()
        self._setup_bm25()
        
    def _load_models(self):
        """ëª¨ë“  í•„ìš”í•œ ëª¨ë¸ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        print("ğŸ“¦ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        
        # ì •ì±… ëª¨ë¸ (QR & Selectorìš©) - ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì‚¬ìš©
        print(f"ğŸ”„ Policy ëª¨ë¸ ë¡œë“œ: {Config.POLICY_MODEL_NAME}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            'meta-llama/Llama-3.2-1B-Instruct',
            use_fast=False
        )
        self.tokenizer.padding_side = "left"
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # 8bit ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=Config.LOAD_IN_8BIT
        )
        
        self.policy_model = AutoModelForCausalLM.from_pretrained(
            Config.POLICY_MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            low_cpu_mem_usage=Config.LOW_CPU_MEM_USAGE,
            use_auth_token=HF_TOKEN
        )
        self.policy_model.eval()
        self.policy_model.config.pad_token_id = self.tokenizer.eos_token_id
        
        # ìƒì„± ëª¨ë¸
        print(f"ğŸ”„ Generator ëª¨ë¸ ë¡œë“œ: {Config.GENERATOR_MODEL_NAME}")
        self.generator_tokenizer = AutoTokenizer.from_pretrained(
            Config.GENERATOR_MODEL_NAME, 
            use_fast=False
        )
        self.generator_model = AutoModelForCausalLM.from_pretrained(
            Config.GENERATOR_MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            low_cpu_mem_usage=Config.LOW_CPU_MEM_USAGE,
            use_auth_token=HF_TOKEN
        )
        self.generator_model.eval()
        
        if self.generator_tokenizer.pad_token is None:
            self.generator_tokenizer.pad_token = self.generator_tokenizer.eos_token
            
        # SBERT ëª¨ë¸
        print(f"ğŸ”„ SBERT ëª¨ë¸ ë¡œë“œ: {Config.SBERT_MODEL_NAME}")
        self.sbert = SentenceTransformer(Config.SBERT_MODEL_NAME)
        
        print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
    def _load_corpus(self):
        """ì½”í¼ìŠ¤ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        print("ğŸ“š ì½”í¼ìŠ¤ ë¡œë“œ ì¤‘...")
        
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if Config.METADB_PATH.startswith("./"):
            corpus_path = project_root / Config.METADB_PATH[2:]
        else:
            corpus_path = Path(Config.METADB_PATH)
        
        self.corpus = []
        try:
            with open(corpus_path, "r", encoding="utf-8") as f:
                for line in f:
                    data = json.loads(line)
                    paper_id = data.get("arxiv_id", data.get("id"))
                    title = data.get("title", "")
                    abstract = data.get("text", "")
                    
                    if paper_id and title and abstract:
                        self.corpus.append({
                            "paper_id": paper_id,
                            "title": title,
                            "abstract": abstract
                        })
        except FileNotFoundError:
            print(f"âŒ ì½”í¼ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {corpus_path}")
            raise
        
        # ì „ì²´ ë¬¸ì„œ ìƒì„± (paper_idë³„ë¡œ ëª¨ë“  ì²­í¬ í•©ì¹˜ê¸°)
        tmp = defaultdict(list)
        for entry in self.corpus:
            pid = entry["paper_id"]
            tmp[pid].append(entry["abstract"])
        self.full_docs = {pid: " ".join(chunks) for pid, chunks in tmp.items()}
        
        print(f"âœ… {len(self.corpus)}ê°œì˜ ë…¼ë¬¸ ë¡œë“œ ì™„ë£Œ")
    
    def _setup_bm25(self):
        """BM25 ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        print("ğŸ” BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        self.chunk_to_pid = [entry["paper_id"] for entry in self.corpus]
        corpus_texts = [entry["abstract"].lower().split() for entry in self.corpus]
        self.bm25 = BM25Okapi(corpus_texts)
        
        print("âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")
    
    def make_qr_prompts(self, questions):
        """QRìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        prompts = []
        for q in questions:
            p = (
                f"{Config.QR_SYSTEM_PROMPT}\n"
                f"{Config.QR_INSTRUCTION}"
                f"input: given question -> {q}\n"
                "response:"
            )
            prompts.append(p)
        return prompts
    
    def make_sel_prompts_strict(self, questions, candidates_batch):
        """Selectorìš© í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        prompts = []
        for q, candidates in zip(questions, candidates_batch):
            cand_lines = [f"{pid}: {title}" for pid, title in candidates]
            cand_text = "\n".join(cand_lines)
            
            p = (
                f"{Config.SEL_SYSTEM_PROMPT}\n"
                f"{Config.SEL_INSTRUCTION}\n\n"
                f"Question: {q}\n"
                f"Candidates:\n{cand_text}\n"
                "Answer:"
            )
            prompts.append(p)
        return prompts
    
    def query_rewrite(self, question):
        """ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•´ ì§ˆë¬¸ ì¬ì‘ì„±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        prompts = self.make_qr_prompts([question])
        
        toks = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=Config.MAX_INPUT_LENGTH
        ).to(self.device)
        
        with torch.no_grad():
            out = self.policy_model.generate(
                **toks,
                do_sample=False,
                num_beams=1,
                max_new_tokens=Config.MAX_NEW_TOKENS_QR,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        prompt_len = toks["input_ids"].shape[-1]
        gen_ids = out[:, prompt_len:]
        decoded = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]
        qr_text = decoded.strip().split("\n")[0].strip()
        
        return qr_text
    
    def retrieve_documents(self, question, qr_text):
        """BM25ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        # ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
        q_tokens = question.lower().split()
        scores_q = self.bm25.get_scores(q_tokens)
        top_q_idx = np.argsort(scores_q)[-Config.K_RETRIEVE:][::-1]
        
        # ì¬ì‘ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰
        sq_tokens = qr_text.lower().split()
        scores_sq = self.bm25.get_scores(sq_tokens)
        top_sq_idx = np.argsort(scores_sq)[-Config.K_RETRIEVE:][::-1]
        
        # ê²°í•©í•˜ì—¬ í›„ë³´ ë…¼ë¬¸ ì¶”ì¶œ
        all_chunk_ids = np.concatenate((top_q_idx, top_sq_idx), axis=0)
        seen = set()
        candidates = []
        
        for cid in all_chunk_ids:
            if cid < len(self.chunk_to_pid):  # ì¸ë±ìŠ¤ ë²”ìœ„ í™•ì¸
                pid = self.chunk_to_pid[cid]
                title = self.corpus[cid]["title"]
                if pid not in seen:
                    seen.add(pid)
                    candidates.append((pid, title))
        
        return candidates
    
    def select_documents(self, question, candidates):
        """Selector ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë¬¸ì„œë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
        prompts = self.make_sel_prompts_strict([question], [candidates])
        
        toks = self.tokenizer(
            prompts, 
            return_tensors="pt",
            padding=True, 
            truncation=True, 
            max_length=Config.MAX_SEL_INPUT_LENGTH
        ).to(self.device)
        
        with torch.no_grad():
            out = self.policy_model.generate(
                **toks,
                max_new_tokens=32,
                do_sample=False,
                num_beams=1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        prompt_len = toks["input_ids"].size(1)
        gen_ids = out[:, prompt_len:]
        decoded = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()
        
        # ID ì¶”ì¶œ
        sel_ids = re.findall(r"\d{4}\.\d{5}", decoded)
        
        # ë¶€ì¡±í•œ ê²½ìš° í›„ë³´ì—ì„œ ë³´ì¶©
        if len(sel_ids) < Config.K_SELECT:
            for pid, _ in candidates:
                if pid not in sel_ids:
                    sel_ids.append(pid)
                if len(sel_ids) == Config.K_SELECT:
                    break
        
        return sel_ids[:Config.K_SELECT]
    
    def generate_answer(self, question, selected_ids):
        """ì„ íƒëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        # ì„ íƒëœ ë¬¸ì„œë“¤ ì¤€ë¹„
        docs = []
        for sid in selected_ids:
            full_text = self.full_docs.get(sid, "")
            # candidatesì—ì„œ ì œëª© ì°¾ê¸°
            title = next((t for (p, t) in self.candidates if p == sid), f"Paper {sid}")
            docs.append(f"[ArXiv:{sid}] {title}. {full_text}")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_msg = Config.DEFAULT_SYSTEM
        doc_section = ""
        for idx_d, d in enumerate(docs):
            doc_section += f"Document{idx_d}: {d}\n"
        
        user_prompt = f"Question: {question}\n{doc_section}Answer:"
        concat_prompt = system_msg + "\n" + user_prompt
        
        # í† í¬ë‚˜ì´ì§•
        inputs = self.generator_tokenizer(
            [concat_prompt],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=Config.MAX_GEN_INPUT_LENGTH
        ).to(self.device)
        
        # ë‹µë³€ ìƒì„±
        with torch.no_grad():
            out = self.generator_model.generate(
                **inputs,
                max_new_tokens=Config.MAX_NEW_TOKENS_GEN,
                temperature=Config.TEMPERATURE,
                do_sample=True if Config.TEMPERATURE > 0 else False,
                pad_token_id=self.generator_tokenizer.pad_token_id,
                eos_token_id=self.generator_tokenizer.eos_token_id,
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        input_len = inputs["input_ids"].shape[1]
        generated_ids = out[0, input_len:]
        answer = self.generator_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
        
        return answer, docs

# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤
_pipeline = None

def get_pipeline():
    """ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _pipeline
    if _pipeline is None:
        print("ğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        _pipeline = RAGPipeline()
        print("âœ… RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!")
    return _pipeline

def infer_attention_question(question):
    """
    ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•´ ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        question (str): ì…ë ¥ ì§ˆë¬¸
        
    Returns:
        dict: ë‹¤ìŒ í‚¤ë¥¼ í¬í•¨í•˜ëŠ” ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            - qr_text: ì¬ì‘ì„±ëœ ì§ˆë¬¸
            - sel_ids: ì„ íƒëœ ë¬¸ì„œ IDë“¤
            - final_answer: ìµœì¢… ë‹µë³€
            - documents: ì‚¬ìš©ëœ ë¬¸ì„œë“¤
    """
    pipeline = get_pipeline()
    
    print(f"ğŸ” ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {question}")
    
    # 1. ì§ˆë¬¸ ì¬ì‘ì„±
    print("ğŸ”„ 1ë‹¨ê³„: ì§ˆë¬¸ ì¬ì‘ì„± ì¤‘...")
    qr_text = pipeline.query_rewrite(question)
    print(f"âœ… ì¬ì‘ì„±ëœ ì§ˆë¬¸: {qr_text}")
    
    # 2. ë¬¸ì„œ ê²€ìƒ‰
    print("ğŸ“š 2ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    candidates = pipeline.retrieve_documents(question, qr_text)
    pipeline.candidates = candidates  # generate_answerì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥
    print(f"âœ… {len(candidates)}ê°œ í›„ë³´ ë¬¸ì„œ ë°œê²¬")
    
    # 3. ë¬¸ì„œ ì„ íƒ
    print("ğŸ¯ 3ë‹¨ê³„: ìµœì  ë¬¸ì„œ ì„ íƒ ì¤‘...")
    selected_ids = pipeline.select_documents(question, candidates)
    print(f"âœ… ì„ íƒëœ ë¬¸ì„œ: {selected_ids}")
    
    # 4. ë‹µë³€ ìƒì„±
    print("ğŸ¤– 4ë‹¨ê³„: ë‹µë³€ ìƒì„± ì¤‘...")
    final_answer, documents = pipeline.generate_answer(question, selected_ids)
    print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
    
    return {
        "qr_text": qr_text,
        "sel_ids": selected_ids,
        "final_answer": final_answer,
        "documents": documents
    } 